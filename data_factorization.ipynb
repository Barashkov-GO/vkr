{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:42.032961672Z",
     "start_time": "2023-05-29T10:26:42.032591442Z"
    }
   },
   "outputs": [],
   "source": [
    "FIGURES_PATH = 'out/figures/'\n",
    "DATASETS_PATH = 'out/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:42.773264937Z",
     "start_time": "2023-05-29T10:26:42.033557770Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocesspandas import applyparallel\n",
    "from pandarallel import pandarallel\n",
    "import psutil\n",
    "from sys import getsizeof\n",
    "\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:42.777375321Z",
     "start_time": "2023-05-29T10:26:42.776406480Z"
    }
   },
   "outputs": [],
   "source": [
    "NROWS = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T10:26:43.674967318Z",
     "start_time": "2023-05-29T10:26:42.780018754Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATASETS_PATH + 'data_processed.csv', nrows=NROWS).drop(columns=['Unnamed: 0'])\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(x):\n",
    "    ans = dict()\n",
    "    for i in x['product_id'].values:\n",
    "        if i in ans:\n",
    "            ans[i] += 1\n",
    "        else:\n",
    "            ans[i] = 1\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_user_purchases(data):\n",
    "    \"\"\"\n",
    "    :param data: receipts - pandas.DataFrame\n",
    "    :return: ans: ans[i][j] = count of purchases by the user i of the product j - matrix\n",
    "    \"\"\"\n",
    "    ans = dict()\n",
    "    data = data[['gid', 'product_id']]\n",
    "\n",
    "\n",
    "    pandarallel.initialize(progress_bar=True, use_memory_fs=True, nb_workers=psutil.cpu_count(logical=False))\n",
    "    ans = data.groupby(by='gid').parallel_apply(process_batch)\n",
    "\n",
    "    return ans.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_users_with_some(data, some=1, unique=False):\n",
    "\n",
    "    pandarallel.initialize(progress_bar=True, use_memory_fs=True, nb_workers=psutil.cpu_count(logical=False))\n",
    "    if unique:\n",
    "        ans = data.groupby(by='gid')[['gid', 'product_id']].parallel_apply(lambda x: x['product_id'].nunique())\n",
    "    else:\n",
    "        ans = data.groupby(by='gid')[['gid', 'product_id']].parallel_apply(lambda x: x.shape[0])\n",
    "\n",
    "    ans = ans.loc[ans >= some].index.array\n",
    "\n",
    "    if len(ans) != 0:\n",
    "        ans = data.loc[~data['gid'].isin(ans)]\n",
    "    else:\n",
    "        ans = data\n",
    "\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = delete_users_with_some(data, some=1, unique=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4538d35b4f6049d099f2081289985b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=14928), Label(value='0 / 14928')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "up = get_user_purchases(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(DATASETS_PATH + 'user_purchases.pkl', 'wb') as f:\n",
    "    pickle.dump(up, f)\n",
    "    \n",
    "del up\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T10:27:04.509996595Z",
     "start_time": "2023-05-29T10:27:04.502003231Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 150_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T10:27:04.946133467Z",
     "start_time": "2023-05-29T10:27:04.943556454Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_date_distances_map(data, interval=None):\n",
    "    \"\"\"\n",
    "    Считаем по каждому пользователю ближайшие (по модулю даты) покупки товаров.\n",
    "    Усредняем значения по каждому пользователю.\n",
    "\n",
    "    :param data: предобработанные данные\n",
    "    :return: ans: ans[(product_1, product_2)] = массив средних временных промежутков для каждого пользователя при покупке товаров\n",
    "    \"\"\"\n",
    "    ans = dict()\n",
    "\n",
    "\n",
    "    def data_splitting(interval):\n",
    "        nonlocal data\n",
    "        batches = []\n",
    "        data = data.sort_values(by='datetime')\n",
    "        start = data.iloc[0].at['datetime']\n",
    "        end = data.iloc[-1].at['datetime']\n",
    "        while start <= end:\n",
    "            sub_end = start + timedelta(days=interval)\n",
    "            batch = data.loc[data['datetime'] >= start].loc[data['datetime'] < sub_end]\n",
    "            batches.append(batch)\n",
    "            start = sub_end\n",
    "\n",
    "        return batches\n",
    "\n",
    "    \n",
    "    def do_dataframe(temps):\n",
    "        ans = pd.DataFrame(data=temps, columns=['product_1', 'product_2', 'timedelta'])\n",
    "        ans['count'] = pd.Series(data=[1 for _ in range(ans.shape[0])])\n",
    "        return ans\n",
    "\n",
    "\n",
    "    def fill_ans(x):\n",
    "        product_date = x[['product_id', 'datetime']]\n",
    "        res = dict()\n",
    "        for i1, r1 in product_date.iterrows():\n",
    "                for i2, r2 in product_date.iterrows():\n",
    "                    if i1 != i2:\n",
    "                        p1, p2 = r1['product_id'], r2['product_id']\n",
    "                        timedelta = (r1['datetime'] - r2['datetime']).days\n",
    "                        \n",
    "                        if (p1, p2) in res:\n",
    "                            if abs(res[(p1, p2)]) > abs(timedelta):\n",
    "                                res[(p1, p2)] = timedelta\n",
    "                        \n",
    "                        else:\n",
    "                            res[(p1, p2)] = timedelta\n",
    "        return res\n",
    "    \n",
    "\n",
    "    def concat_dicts(res):\n",
    "        nonlocal ans\n",
    "        res = res.values\n",
    "        for r in res:\n",
    "            for key in r.keys():\n",
    "                if key in ans:\n",
    "                    ans[key].append(r[key])\n",
    "                else:\n",
    "                    ans[key] = [r[key]]\n",
    "                \n",
    "        return ans\n",
    "        \n",
    "\n",
    "    data = data[['gid', 'product_id', 'datetime']]\n",
    "    data.loc[:, 'datetime'] = data['datetime'].dt.date\n",
    "    if interval is not None:\n",
    "        batches = data_splitting(interval=interval)\n",
    "    else:\n",
    "        batches = np.array_split(data, data.shape[0] // BATCH_SIZE + 1)\n",
    "    \n",
    "    pandarallel.initialize(progress_bar=False, use_memory_fs=True, nb_workers=psutil.cpu_count(logical=False))\n",
    "    for batch in tqdm(batches):\n",
    "        if psutil.virtual_memory().percent >= 90:\n",
    "            break\n",
    "        grouped_by_user = batch.groupby(by='gid')\n",
    "        temp = grouped_by_user.parallel_apply(fill_ans)\n",
    "        temp = temp.dropna()\n",
    "        ans = concat_dicts(temp)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T10:31:56.870335399Z",
     "start_time": "2023-05-29T10:27:06.320150221Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7e52c2850a431ca9b517df4fbb410b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dists_map = get_date_distances_map(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dists_map\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T10:43:16.523978533Z",
     "start_time": "2023-05-29T10:43:16.522414502Z"
    }
   },
   "outputs": [],
   "source": [
    "# переписать на групбай\n",
    "def process_batch(batch):\n",
    "    res = dict()\n",
    "    for key in tqdm(batch.keys()):\n",
    "        arr = np.array(batch[key])\n",
    "        res[key] = [arr.mean(), arr.shape[0], np.quantile(arr, 0.75) - np.quantile(arr, 0.25)]\n",
    "    return res\n",
    "\n",
    "\n",
    "def concat_batches(dist):\n",
    "    trans_dists.update(dist)\n",
    "    \n",
    "\n",
    "def transform_dists(dists):\n",
    "    \n",
    "    def chunks(dictionary, size):\n",
    "        items = list(dictionary.items())\n",
    "        return [dict(items[i : i + size]) for i in range(0, len(items), size)]\n",
    "\n",
    "    def custom_error_callback(error):\n",
    "        print(f'Got an Error: {error}', flush=True)\n",
    "    \n",
    "    \n",
    "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
    "    batches = chunks(dists, len(dists) // psutil.cpu_count() // 2)\n",
    "    \n",
    "#     del dists\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    for batch in batches:\n",
    "        pool.apply_async(\n",
    "            process_batch,\n",
    "            args=(batch,),\n",
    "            callback=concat_batches,\n",
    "            error_callback=custom_error_callback,\n",
    "        )\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T10:48:23.960922815Z",
     "start_time": "2023-05-29T10:43:16.526902728Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "trans_dists = dict()\n",
    "transform_dists(dists_map)\n",
    "print('Done')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T10:43:07.354877324Z",
     "start_time": "2023-05-29T10:43:07.111196798Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trans_dists' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrans_dists\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trans_dists' is not defined"
     ]
    }
   ],
   "source": [
    "trans_dists"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             _: 320.0 MiB\n",
      "                     dists_map: 320.0 MiB\n",
      "                   trans_dists: 320.0 MiB\n",
      "                           _17: 320.0 MiB\n",
      "                          data: 89.1 MiB\n",
      "                          _i11:  8.0 KiB\n",
      "                          _iii:  3.2 KiB\n",
      "                          _i15:  3.2 KiB\n",
      "                          tqdm:  2.0 KiB\n",
      "                   pandarallel:  1.0 KiB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n",
    "                          globals().items())), key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASETS_PATH + 'date_distances.pkl', 'wb') as f:\n",
    "    pickle.dump(trans_dists, f)\n",
    "    \n",
    "del trans_dists"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
