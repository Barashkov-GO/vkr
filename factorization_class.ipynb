{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "FIGURES_PATH = 'out/figures/'\n",
    "DATASETS_PATH = 'out/datasets/'\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import multiprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "from pandarallel import pandarallel\n",
    "import psutil\n",
    "\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "tqdm.pandas()\n",
    "from helper import *\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T22:40:09.368445893Z",
     "start_time": "2023-06-04T22:40:08.695256689Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-04T23:01:52.153324295Z",
     "start_time": "2023-06-04T23:01:52.106701120Z"
    }
   },
   "outputs": [],
   "source": [
    "class Distances:\n",
    "    def __init__(self, data_path='data_processed', nrows=None):\n",
    "        \"\"\"\n",
    "        Initialize Distances class with the data\n",
    "\n",
    "        :param data_path: name of file\n",
    "        :param nrows: number of rows to read\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(DATASETS_PATH + data_path + '.csv', nrows=nrows).drop(columns=['Unnamed: 0'])\n",
    "        self.data['datetime'] = pd.to_datetime(self.data['datetime'])\n",
    "        self.nrows = nrows\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dists(file, path):\n",
    "        with open(DATASETS_PATH + path + '.pkl', 'wb') as f:\n",
    "            pickle.dump(file, f)\n",
    "\n",
    "\n",
    "    def top_users(self, top_lim, field='product_id'):\n",
    "        data = self.data[['gid', field]]\n",
    "        d = data.groupby(by='gid').apply(lambda x: len(x)).sort_values(ascending=False)\n",
    "        return data.loc[data['gid'].isin(d.index.values[:top_lim])]\n",
    "\n",
    "\n",
    "    def top_products(self, top_lim, field='product_id'):\n",
    "        data = self.data[[field, 'datetime', 'gid']]\n",
    "        d = data.groupby(by=field).apply(lambda x: len(x)).sort_values(ascending=False)\n",
    "        return data.loc[data[field].isin(d.index.values[:top_lim])]\n",
    "\n",
    "\n",
    "    def user_product(self, top_lim=None, field='product_id'):\n",
    "        \"\"\"\n",
    "        Get distances between all pairs of users by counting purchases\n",
    "\n",
    "        :return: dict[user] = {product: count}\n",
    "        \"\"\"\n",
    "\n",
    "        def process_batch(x):\n",
    "            ans = dict()\n",
    "            for i in x[field].values:\n",
    "                if i in ans:\n",
    "                    ans[i] += 1\n",
    "                else:\n",
    "                    ans[i] = 1\n",
    "            return ans\n",
    "\n",
    "\n",
    "        data = self.top_users(top_lim, field)\n",
    "        print(len(data['gid'].drop_duplicates()))\n",
    "\n",
    "        pandarallel.initialize(progress_bar=True, use_memory_fs=True, nb_workers=psutil.cpu_count(logical=False))\n",
    "        ans = data.groupby(by='gid').parallel_apply(process_batch)\n",
    "\n",
    "        print(len(ans))\n",
    "        ans = dict(ans)\n",
    "        print(len(ans))\n",
    "        self.save_dists(ans, f'up_{field}_{self.nrows}')\n",
    "        return ans\n",
    "\n",
    "    def product_product(self, top_lim=None, interval=None, batch_size=100_000, field='product_id'):\n",
    "        \"\"\"\n",
    "        Get distances between all pairs of products by date differences\n",
    "\n",
    "        :param interval: date interval to split data with, default: None\n",
    "        :param batch_size: data batching size, default: 100_000\n",
    "        :return: dict[(product_1, product_2)] = an array of mean of date distance by one user\n",
    "        \"\"\"\n",
    "        ans = dict()\n",
    "\n",
    "        data = self.top_products(top_lim=top_lim, field=field)\n",
    "\n",
    "        print(len(data[field].drop_duplicates()))\n",
    "        data.loc[:, 'datetime'] = data['datetime'].dt.date\n",
    "\n",
    "        def data_splitting(interval):\n",
    "            nonlocal data\n",
    "            batches = []\n",
    "            data = data.sort_values(by='datetime')\n",
    "            start = data.iloc[0].at['datetime']\n",
    "            end = data.iloc[-1].at['datetime']\n",
    "            while start <= end:\n",
    "                sub_end = start + timedelta(days=interval)\n",
    "                batch = data.loc[data['datetime'] >= start].loc[data['datetime'] < sub_end]\n",
    "                batches.append(batch)\n",
    "                start = sub_end\n",
    "\n",
    "            return batches\n",
    "\n",
    "\n",
    "        def fill_ans(x):\n",
    "            product_date = x[[field, 'datetime']]\n",
    "            res = dict()\n",
    "            for i1, r1 in product_date.iterrows():\n",
    "                for i2, r2 in product_date.iterrows():\n",
    "                    if i1 != i2:\n",
    "                        p1, p2 = r1[field], r2[field]\n",
    "                        timedelta = (r1['datetime'] - r2['datetime']).days\n",
    "\n",
    "                        if (p1, p2) in res and abs(res[(p1, p2)]) > abs(timedelta):\n",
    "                            res[(p1, p2)] = timedelta\n",
    "                        else:\n",
    "                            res[(p1, p2)] = timedelta\n",
    "            return res\n",
    "\n",
    "\n",
    "        def concat_dicts(res):\n",
    "            nonlocal ans\n",
    "            res = res.values\n",
    "            for r in res:\n",
    "                for key in r.keys():\n",
    "                    if key in ans:\n",
    "                        ans[key].append(r[key])\n",
    "                    else:\n",
    "                        ans[key] = [r[key]]\n",
    "            return ans\n",
    "\n",
    "        if interval is not None:\n",
    "            batches = data_splitting(interval=interval)\n",
    "        else:\n",
    "            batches = np.array_split(data, data.shape[0] // batch_size + 1)\n",
    "\n",
    "        pandarallel.initialize(progress_bar=False, use_memory_fs=True, nb_workers=psutil.cpu_count(logical=False))\n",
    "\n",
    "        for batch in tqdm(batches):\n",
    "            if psutil.virtual_memory().percent >= 90:\n",
    "                break\n",
    "            grouped_by_user = batch.groupby(by='gid')\n",
    "            temp = grouped_by_user.parallel_apply(fill_ans)\n",
    "            temp = temp.dropna()\n",
    "            ans = concat_dicts(temp)\n",
    "\n",
    "        self.save_dists(ans, f'pp_{field}_{self.nrows}')\n",
    "        return ans\n",
    "\n",
    "\n",
    "    def get_up_matrix(self, field='product_id', top_lim=None, batch_size=100_000):\n",
    "        def fill_ans(x):\n",
    "            nonlocal ans\n",
    "            user, product = x[0], x[1]\n",
    "            ans[user, product] += 1\n",
    "\n",
    "        data = self.top_users(top_lim=top_lim, field=field)\n",
    "\n",
    "\n",
    "        max_user, max_product = max(list(data['gid'].values)), max(list(data[field].values))\n",
    "        print(max_user)\n",
    "        print(max_product)\n",
    "        ans = np.full((max_user, max_product), 0)\n",
    "        for batch in tqdm(np.array_split(data, data.shape[0] // batch_size)):\n",
    "            batch.apply(fill_ans, axis=1)\n",
    "\n",
    "        self.save_dists(ans, f'up_matrix_{field}_{self.nrows}')\n",
    "        return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "d = Distances(nrows=5_000_000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T23:01:58.816674430Z",
     "start_time": "2023-06-04T23:01:53.119493927Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp = d.product_product(batch_size=200_000, top_lim=20_000, field='category_id')\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-04T23:01:58.831237568Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del pp"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "up = d.user_product(field='category_id', top_lim=10_000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(len(up))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
