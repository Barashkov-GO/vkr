{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:30.325753694Z",
     "start_time": "2023-05-31T00:47:30.320183217Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "FIGURES_PATH = 'out/figures/'\n",
    "DATASETS_PATH = 'out/datasets/'\n",
    "DICTS_PATH = 'out/dicts/'\n",
    "CLUSTERS_PATH = 'out/clusters/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.332835688Z",
     "start_time": "2023-05-31T00:47:30.320308783Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocesspandas import applyparallel\n",
    "from pandarallel import pandarallel\n",
    "import psutil\n",
    "from sys import getsizeof\n",
    "import networkx as nx\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "\n",
    "from netgraph import Graph, InteractiveGraph, EditableGraph\n",
    "\n",
    "import pickle\n",
    "import gc \n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.347144713Z",
     "start_time": "2023-05-31T00:47:31.334028148Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open(DATASETS_PATH + 'date_distances.pkl', 'rb') as f:\n",
    "#     dists = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.347335121Z",
     "start_time": "2023-05-31T00:47:31.337783715Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open(DATASETS_PATH + 'user_purchases.pkl', 'rb') as f:\n",
    "#     dists = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.347402598Z",
     "start_time": "2023-05-31T00:47:31.342942377Z"
    }
   },
   "outputs": [],
   "source": [
    "# dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.391009542Z",
     "start_time": "2023-05-31T00:47:31.346132371Z"
    }
   },
   "outputs": [],
   "source": [
    "# dict(sorted(list(dists.items()), key=(lambda x: x[1][1]), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.391349931Z",
     "start_time": "2023-05-31T00:47:31.388233346Z"
    }
   },
   "outputs": [],
   "source": [
    "# list(dists.items()).sort(key=(lambda x: x[1][0]))\n",
    "# (product_1, product_2) - [mean data distance, count, quartile range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.391476760Z",
     "start_time": "2023-05-31T00:47:31.388354393Z"
    }
   },
   "outputs": [],
   "source": [
    "def default(mean, count, scatter):\n",
    "    return (mean + abs(scatter)) / (count ** 2)\n",
    "\n",
    "def get_dists(dists, count_lower=10, dist_func=default):\n",
    "    return dict([(i[0], dist_func(i[1][0], i[1][1], i[1][2])) \n",
    "                 for i in dists.items() \n",
    "                 if (i[1][1] >= count_lower\n",
    "                     or dist_func(i[1][0], i[1][1], i[1][2]) != 0) \n",
    "                 and dist_func(i[1][0], i[1][1], i[1][2]) >= 0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.391589021Z",
     "start_time": "2023-05-31T00:47:31.388407433Z"
    }
   },
   "outputs": [],
   "source": [
    "# dists = get_dists(dists, count_lower=30, dist_func=default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.391698747Z",
     "start_time": "2023-05-31T00:47:31.388451365Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open(CLUSTERS_PATH + 'dists.pkl', 'wb') as f:\n",
    "#     pickle.dump(dists, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.391899384Z",
     "start_time": "2023-05-31T00:47:31.388490259Z"
    }
   },
   "outputs": [],
   "source": [
    "def k_means_clustering(dists, k, max_iterations=100):\n",
    "    \n",
    "    def dist_between_products(product1, product2):\n",
    "        if product1 == product2:\n",
    "            return 0\n",
    "\n",
    "        if (product1, product2) in dists:\n",
    "            return dists[(product1, product2)]\n",
    "\n",
    "        if (product2, product1) in dists:\n",
    "            return dists[(product2, product1)]\n",
    "\n",
    "        return float('inf')\n",
    "    \n",
    "    \n",
    "    def comp(product, mini):\n",
    "        return dist_between_products(product, mini)\n",
    "    \n",
    "    \n",
    "    def get_dist_between(product, cluster):\n",
    "        dist = 0.0\n",
    "        cnt = 0\n",
    "        for c in cluster:\n",
    "            if (product, c) in dists:\n",
    "                dist += dists[(product, c)]\n",
    "                cnt += 1\n",
    "            elif (c, product) in dists:\n",
    "                dist += dists[(c, product)]\n",
    "                cnt += 1\n",
    "        if cnt == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        return dist / cnt\n",
    "    \n",
    "    def clear_clusters(clusters):\n",
    "        for cluster in clusters:\n",
    "        \n",
    "            mini = (float('inf'), 0)\n",
    "            for i, p in enumerate(cluster):\n",
    "                mean_dist = get_dist_between(p, cluster)\n",
    "                if mean_dist < mini[0]:\n",
    "                    mini = (mean_dist, p)\n",
    "\n",
    "            cluster = sorted(cluster, key=(lambda x: comp(x, mini[1])), reverse=False)\n",
    "            cluster = [p for p in cluster if dist_between_products(p, mini[1]) < float('inf')]\n",
    "        \n",
    "        clusters = [i for i in clusters if len(i) > 0]\n",
    "        return clusters\n",
    "        \n",
    "        \n",
    "    products = np.unique(np.concatenate(list(dists.keys())))\n",
    "    \n",
    "    clusters = np.random.choice(products, k, replace=False)\n",
    "    clusters = [[c] for c in clusters]\n",
    "    products = products[~np.isin(products, clusters)]\n",
    "    \n",
    "    \n",
    "    mi_break = False\n",
    "    ri_break = False\n",
    "    \n",
    "    print('Starting products splitting to clusters...')\n",
    "    for p in tqdm(products):\n",
    "        p_dist = [get_dist_between(p, c) for c in clusters]\n",
    "        pos = np.argmin(p_dist)\n",
    "        clusters[pos].append(p)\n",
    "        products = products[products != p]\n",
    "            \n",
    "    \n",
    "    \n",
    "    print('Starting operating over clusters...')\n",
    "    for _ in range(max_iterations):\n",
    "        \n",
    "        clusters_prev = clusters\n",
    "        \n",
    "        for c in tqdm(clusters):\n",
    "            for p in c:\n",
    "                p_dist = [get_dist_between(p, c_other) for c_other in clusters]\n",
    "                pos = np.argmin(p_dist)\n",
    "                c.remove(p)\n",
    "                clusters[pos].append(p)\n",
    "                \n",
    "                \n",
    "        if clusters_prev == clusters:\n",
    "            print('Clusters stabilizied!')\n",
    "            ri_break = True\n",
    "            break\n",
    "            \n",
    "    if not ri_break:\n",
    "        print('Stopped for maximum of iterations: {}'.format(max_iterations))\n",
    "    \n",
    "    clusters = clear_clusters(clusters)\n",
    "\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.392052531Z",
     "start_time": "2023-05-31T00:47:31.388597821Z"
    }
   },
   "outputs": [],
   "source": [
    "# clusters = k_means_clustering(dists, k=100, max_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.441177583Z",
     "start_time": "2023-05-31T00:47:31.388642174Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open(CLUSTERS_PATH + 'k_means.pkl','wb') as f:\n",
    "#      pickle.dump(clusters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.441393669Z",
     "start_time": "2023-05-31T00:47:31.432194886Z"
    }
   },
   "outputs": [],
   "source": [
    "# clusters_ward = ward_clustering(dists, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T00:47:31.441468881Z",
     "start_time": "2023-05-31T00:47:31.432300274Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open(CLUSTERS_PATH + 'ward.pkl','wb') as f:\n",
    "#      pickle.dump(clusters_ward, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T01:07:13.864684013Z",
     "start_time": "2023-05-31T01:07:13.823690170Z"
    }
   },
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    def __init__(self, method='euclidean', max=100):\n",
    "        self.method = method\n",
    "        self.max = max\n",
    "\n",
    "    def run(self, cluster1, cluster2, dists):\n",
    "        self.cluster1 = cluster1\n",
    "        self.cluster2 = cluster2\n",
    "        self.dists = dists\n",
    "\n",
    "        if self.method == 'euclidean':\n",
    "            return self.euclidean()\n",
    "        if self.method == 'min_dist':\n",
    "            return self.min_dist()\n",
    "        if self.method == 'max_dist':\n",
    "            return self.max_dist()\n",
    "        if self.method == 'average':\n",
    "            return self.average()\n",
    "        if self.method == 'ward':\n",
    "            return self.ward()\n",
    "\n",
    "    def _get(self, i, j):\n",
    "        if i == j:\n",
    "            return 0.0\n",
    "        if (i, j) in self.dists:\n",
    "            return self.dists[(i, j)]\n",
    "        if (j, i) in self.dists:\n",
    "            return self.dists[(j, i)]\n",
    "        return self.max\n",
    "\n",
    "\n",
    "    def euclidean(self):\n",
    "        n1, n2 = len(self.cluster1), len(self.cluster2)\n",
    "        s = 0.0\n",
    "        for i in self.cluster1:\n",
    "            for j in self.cluster2:\n",
    "                s += self._get(i, j) ** 2\n",
    "        return np.sqrt(s)\n",
    "\n",
    "\n",
    "    def min_dist(self):\n",
    "        s, mini = 0.0, self.max + 1\n",
    "        for i in self.cluster1:\n",
    "            for j in self.cluster2:\n",
    "                s = self._get(i, j)\n",
    "\n",
    "                if s < mini:\n",
    "                    mini = s\n",
    "        return mini\n",
    "\n",
    "\n",
    "    def max_dist(self):\n",
    "        s, maxi = 0.0, -1.0\n",
    "        for i in self.cluster1:\n",
    "            for j in self.cluster2:\n",
    "                s = self._get(i, j)\n",
    "\n",
    "                if s > maxi:\n",
    "                    maxi = s\n",
    "        return maxi\n",
    "\n",
    "\n",
    "    def average(self):\n",
    "        n1, n2 = len(self.cluster1), len(self.cluster2)\n",
    "        s = 0.0\n",
    "        for i in self.cluster1:\n",
    "            for j in self.cluster2:\n",
    "                s += self._get(i, j)\n",
    "        return s / (n1 * n2)\n",
    "\n",
    "\n",
    "    def ward(self):\n",
    "        n1, n2 = len(self.cluster1), len(self.cluster2)\n",
    "        s_u, s_1, s_2 = 0.0, 0.0, 0.0\n",
    "        for i in self.cluster1:\n",
    "            for j in self.cluster2:\n",
    "                s_u += self._get(i, j) ** 2\n",
    "\n",
    "        for i in range(n1):\n",
    "            for j in range(i + 1, n1):\n",
    "                s_1 += self._get(self.cluster1[i], self.cluster1[j])\n",
    "\n",
    "        for i in range(n2):\n",
    "            for j in range(i + 1, n2):\n",
    "                s_2 += self._get(self.cluster2[i], self.cluster2[j])\n",
    "        return (s_u - s_1 - s_2) / (n1 + n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T01:19:01.661298719Z",
     "start_time": "2023-05-31T01:19:01.616071070Z"
    }
   },
   "outputs": [],
   "source": [
    "class Clustering:\n",
    "    def __init__(self, get_dists=get_dists):\n",
    "        self.get_dists = get_dists\n",
    "        self.statistics = {\n",
    "            'min_distances': [],\n",
    "            'time_of_iter': [],\n",
    "            'time_of_all': 0.0,\n",
    "            'count_of_iters': 0.0,\n",
    "            }\n",
    "\n",
    "    def get_stats(self):\n",
    "        self.statistics['time_of_iter'] = np.array(self.statistics['time_of_iter']).mean()\n",
    "        for k in self.statistics.keys():\n",
    "            print(f\"{k} --- {self.statistics[k]}\")\n",
    "        return self.statistics\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_clusters(cluster1, cluster2):\n",
    "        merged_cluster = cluster1 + cluster2\n",
    "        return merged_cluster\n",
    "\n",
    "    def run(self, dists, k):\n",
    "        start0 = datetime.now()\n",
    "\n",
    "        elements = np.unique(list(dists.keys())[:100_000])\n",
    "        # elements = list(set(list(np.concatenate(dists.keys())[:10_000])))\n",
    "        clusters = [[i] for i in elements]\n",
    "        iters = len(elements) - k\n",
    "\n",
    "        clusters_dists = np.full((len(elements), len(elements)), -1.0)\n",
    "\n",
    "        print('Starting counting distances between clusters...')\n",
    "        for i in tqdm(range(len(clusters))):\n",
    "            for j in range(i + 1, len(clusters)):\n",
    "                distance = self.metric.run(clusters[i], clusters[j], dists)\n",
    "                clusters_dists[i][j] = distance\n",
    "                clusters_dists[j][i] = distance\n",
    "\n",
    "        print('Starting collapsing closest clusters...')\n",
    "        for _ in tqdm(range(iters)):\n",
    "            start = datetime.now()\n",
    "\n",
    "            a = np.argmin(clusters_dists)\n",
    "            i, j = a // clusters_dists.shape[1], a % clusters_dists.shape[1]\n",
    "\n",
    "            min_distance = clusters_dists[i, j]\n",
    "            merged_cluster = self._merge_clusters(clusters[i], clusters[j])\n",
    "            del clusters[j]\n",
    "            del clusters[i]\n",
    "            clusters_dists[i, :] = np.inf\n",
    "            clusters_dists[j, :] = np.inf\n",
    "            clusters_dists[:, i] = np.inf\n",
    "            clusters_dists[:, j] = np.inf\n",
    "\n",
    "            clusters.append(merged_cluster)\n",
    "\n",
    "            j = len(clusters) - 1\n",
    "            for i in range(len(clusters)):\n",
    "                distance = self.metric.run(clusters[i], clusters[j], dists)\n",
    "                clusters_dists[i][j], clusters_dists[j][i] = distance, distance\n",
    "\n",
    "            self.statistics['min_distances'].append(min_distance)\n",
    "            self.statistics['time_of_iter'].append(datetime.now() - start)\n",
    "        self.statistics['count_of_iters'] = iters\n",
    "        self.statistics['time_of_all'] = datetime.now() - start0\n",
    "\n",
    "        return clusters\n",
    "\n",
    "    def run_k_means(self, dists, k, max_iter=10_000):\n",
    "        # Можно наканпливать minimal_dist, как внутрикластерное расстояние (в агломеративных тоже)\n",
    "        # Можно сохранять среднее расстояние между кластерами и внутри кластеров, чтобы показывать на графике\n",
    "\n",
    "        start0 = datetime.now()\n",
    "        elements = np.unique(sorted(list(dists.keys()))[:25_000])\n",
    "\n",
    "        clusters = np.random.choice(elements, k, replace=False)\n",
    "        elements = elements[~np.isin(elements, clusters)]\n",
    "        clusters = [[c] for c in clusters]\n",
    "\n",
    "\n",
    "        print('Starting elements splitting by clusters...')\n",
    "        for e in tqdm(elements):\n",
    "            minimal_dist = self.metric.max * 5.0\n",
    "            cluster_index = 0\n",
    "            for i, c in enumerate(clusters):\n",
    "                dist = self.metric.run([e], c, dists)\n",
    "                if dist < minimal_dist:\n",
    "                    minimal_dist = dist\n",
    "                    cluster_index = i\n",
    "\n",
    "            clusters[cluster_index].append(e)\n",
    "\n",
    "\n",
    "        print('Starting operating over clusters...')\n",
    "        for _ in tqdm(range(max_iter)):\n",
    "            self.statistics['count_of_iters'] += 1\n",
    "            start = datetime.now()\n",
    "            prev_clusters = clusters.copy()\n",
    "            for c1 in clusters:\n",
    "                for pos_el, el in enumerate(c1):\n",
    "                    minimal_dist = self.metric.max * 5.0\n",
    "                    cluster_index = 0\n",
    "                    for i, c in enumerate(clusters):\n",
    "                        dist = self.metric.run([el], c, dists)\n",
    "                        if dist < minimal_dist:\n",
    "                            minimal_dist = dist\n",
    "                            cluster_index = i\n",
    "\n",
    "                    self.statistics['min_distances'].append(minimal_dist)\n",
    "\n",
    "                    del c1[pos_el]\n",
    "                    clusters[cluster_index].append(el)\n",
    "\n",
    "            if prev_clusters == clusters:\n",
    "                print('Clusters stabilizied!')\n",
    "                self.statistics['time_of_all'] = datetime.now() - start0\n",
    "                return clusters\n",
    "\n",
    "            self.statistics['time_of_iter'].append(datetime.now() - start)\n",
    "\n",
    "        print('Stopped for maximum iterations: {}'.format(max_iter))\n",
    "        self.statistics['time_of_all'] = datetime.now() - start0\n",
    "        return clusters\n",
    "\n",
    "\n",
    "    def fit(self, metric, type='aglomerative', dists_path='date_distances', k=10, max_iter=10_000):\n",
    "        with open(DATASETS_PATH + dists_path + '.pkl', 'rb') as f:\n",
    "            self.dists = pickle.load(f)\n",
    "\n",
    "        print('clustering...')\n",
    "\n",
    "        dists = self.get_dists(self.dists)\n",
    "\n",
    "        self.metric = metric\n",
    "\n",
    "        if type == 'aglomerative':\n",
    "            return self.run(dists, k)\n",
    "        else:\n",
    "            return self.run_k_means(dists, k, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T01:19:04.486006971Z",
     "start_time": "2023-05-31T01:19:04.484610027Z"
    }
   },
   "outputs": [],
   "source": [
    "c = Clustering()\n",
    "# clusters_euc = c.fit(metric=Metric('max_dist'), type='k_means', k=10_000, max_iter=10_000)\n",
    "#\n",
    "# with open(CLUSTERS_PATH + 'k_means_max_dist.pkl','wb') as f:\n",
    "#      pickle.dump(clusters_euc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering...\n",
      "Starting counting distances between clusters...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/12201 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c2466d4f41e413da6a481b9827aaa78"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting collapsing closest clusters...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2201 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "275e63576c6d47daa1afe01b0bea594f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clusters_euc = c.fit(metric=Metric('max_dist'), type='aglomerative', k=10_000, max_iter=10_000)\n",
    "\n",
    "\n",
    "with open(CLUSTERS_PATH + 'ward_max_dist.pkl','wb') as f:\n",
    "     pickle.dump(clusters_euc, f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-31T01:19:04.495191518Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T01:18:31.005452265Z",
     "start_time": "2023-05-31T01:18:30.957719740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_distances --- []\n",
      "time_of_iter --- nan\n",
      "time_of_all --- 0:00:05.075247\n",
      "count_of_iters --- -7711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5201/1426523587.py:12: RuntimeWarning: Mean of empty slice.\n",
      "  self.statistics['time_of_iter'] = np.array(self.statistics['time_of_iter']).mean()\n",
      "/home/fanat/Projects/modeling/dz2_new/venv/lib/python3.10/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'min_distances': [],\n 'time_of_iter': nan,\n 'time_of_all': datetime.timedelta(seconds=5, microseconds=75247),\n 'count_of_iters': -7711}"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-31T00:49:18.667078893Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open(CLUSTERS_PATH + 'k_means_euclidean.pkl','wb') as f:\n",
    "#      pickle.dump(clusters_euc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
