{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:49.788009Z",
     "end_time": "2023-04-28T02:08:49.826661Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install matplotlib\n",
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:49.788119Z",
     "end_time": "2023-04-28T02:08:50.866759Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import multiprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.872192Z",
     "end_time": "2023-04-28T02:08:50.873688Z"
    }
   },
   "outputs": [],
   "source": [
    "FIGURES_PATH = 'out/figures/'\n",
    "DATASETS_PATH = 'out/datasets/'\n",
    "DICTS_PATH = 'out/dicts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.876302Z",
     "end_time": "2023-04-28T02:08:50.879074Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv('datasets/receipts_new.csv', nrows=5_000_000)\n",
    "# data = data.rename(columns = {'line_item_id': 'product_id'})\n",
    "# data\n",
    "# data.head(1000).to_csv('datasets/receipts_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.883760Z",
     "end_time": "2023-04-28T02:08:50.896118Z"
    }
   },
   "outputs": [],
   "source": [
    "# data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что совпадают некоторые строки по ключевому признаку - transaction_key, необходимо произвести схлопывание данных по одинаковым ключам с суммированием признаков line_quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение обработанных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.890756Z",
     "end_time": "2023-04-28T02:08:50.953568Z"
    }
   },
   "outputs": [],
   "source": [
    "def save(data, name):\n",
    "    print('SAVING dataset: {}\\n'.format(name.upper()))\n",
    "    data.info(memory_usage='deep')\n",
    "    data.to_csv(DATASETS_PATH + name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Схлопывание одинаковых позиций в чеках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.936161Z",
     "end_time": "2023-04-28T02:08:50.953797Z"
    }
   },
   "outputs": [],
   "source": [
    "def collapse(data):\n",
    "    # print('Collapsing data')\n",
    "    data_grouped = data[['transaction_key', 'product_id', 'line_quantity']].groupby(by = ['transaction_key', 'product_id'])\n",
    "    grouped_series = data_grouped['line_quantity'].agg(lambda x: sum(x))\n",
    "    data = data.join(grouped_series, how='left', on=['transaction_key', 'product_id'], rsuffix='LA')\n",
    "    data = data.drop(columns=['line_quantity'])\n",
    "    data = data.rename(columns={'line_quantityLA': 'line_quantity'})\n",
    "    return data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.936282Z",
     "end_time": "2023-04-28T02:08:50.953872Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = collapse(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование transaction_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.936440Z",
     "end_time": "2023-04-28T02:08:50.953918Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(str):\n",
    "    return str.split('_')[3:4][0]\n",
    "\n",
    "def get_key(str):\n",
    "    splitted = str.split('_')\n",
    "    ans = ''\n",
    "    for i in range(3):\n",
    "        ans += splitted[i]\n",
    "    ans += splitted[-1]\n",
    "    return int(ans)\n",
    "\n",
    "def transform_transaction_key(data, trans_id):\n",
    "    # print('Transforming dates')\n",
    "    dates = data['transaction_key'].apply(get_data)\n",
    "    dates = pd.to_datetime(dates)\n",
    "    data['datetime'] = dates\n",
    "\n",
    "    # print('Transforming keys')\n",
    "    keys = data['transaction_key']\n",
    "    keys = keys.drop_duplicates()\n",
    "    keys = keys.to_frame()\n",
    "\n",
    "    for ind, key in keys.iterrows():\n",
    "        keys.at[ind, 'key_id'] = trans_id.push(key['transaction_key'])\n",
    "\n",
    "    data = data.join(keys.set_index('transaction_key'), how='left', on='transaction_key')\n",
    "    data = data.drop(columns=['transaction_key'])\n",
    "    return data, trans_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.936613Z",
     "end_time": "2023-04-28T02:08:50.953962Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = transform_transaction_key(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование gid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.936786Z",
     "end_time": "2023-04-28T02:08:50.954004Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_gid(data, gids_id):\n",
    "    # print('Transforming gid')\n",
    "    gids = data['gid']\n",
    "    gids = gids.drop_duplicates()\n",
    "    gids = gids.to_frame()\n",
    "\n",
    "    for ind, gid in gids.iterrows():\n",
    "        gids.at[ind, 'gid_id'] = gids_id.push(gid['gid'])\n",
    "\n",
    "    data = data.join(gids.set_index('gid'), on='gid', how='left')\n",
    "    return data.drop(columns=['gid']), gids_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.936990Z",
     "end_time": "2023-04-28T02:08:50.954045Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = transform_gid(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование line_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.984136Z",
     "end_time": "2023-04-28T02:08:51.012941Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_line_type(data, lines_id):\n",
    "    # print('Transforming line_type')\n",
    "    line_types = data['line_type']\n",
    "    line_types = line_types.drop_duplicates()\n",
    "    line_types = line_types.to_frame()\n",
    "\n",
    "    for ind, line_type in line_types.iterrows():\n",
    "        line_types.at[ind, 'line_type_id'] = lines_id.push(line_type['line_type'])\n",
    "\n",
    "    data = data.join(line_types.set_index('line_type'), on='line_type', how='left')\n",
    "    return data.drop(columns=['line_type']), lines_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.984257Z",
     "end_time": "2023-04-28T02:08:51.013088Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = transform_line_type(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подключение категорий товаров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.984350Z",
     "end_time": "2023-04-28T02:08:51.013148Z"
    }
   },
   "outputs": [],
   "source": [
    "def include_categories(data, cats_path='datasets/ProductsModels.csv'):\n",
    "    # print('Including products categories')\n",
    "    cats = pd.read_csv(cats_path)\n",
    "    cats = cats.rename(columns = {'model_id': 'category_id', 'name': 'category_name'})\n",
    "    cats = cats.drop(columns = ['category_name'])\n",
    "    data = data.join(cats.set_index('product_id'), on='product_id', how='left')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.984448Z",
     "end_time": "2023-04-28T02:08:51.013196Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = include_categories(data, 'datasets/ProductsModels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление дней недели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.984524Z",
     "end_time": "2023-04-28T02:08:51.013379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING dataset: WEEKDAYS_KEYS.CSV\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   0           7 non-null      object\n",
      " 1   weekday_id  7 non-null      int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 868.0 bytes\n"
     ]
    }
   ],
   "source": [
    "weekday_dict = {\n",
    "    0: 'Воскресенье',\n",
    "    1: 'Понедельник',\n",
    "    2: 'Вторник',\n",
    "    3: 'Среда',\n",
    "    4: 'Четверг',\n",
    "    5: 'Пятница',\n",
    "    6: 'Суббота',\n",
    "               }\n",
    "\n",
    "weekday_dict.values()\n",
    "weekdays_series = pd.Series(weekday_dict.values())\n",
    "weekdays_series = weekdays_series.to_frame()\n",
    "weekdays_series['weekday_id'] = weekdays_series.index\n",
    "\n",
    "save(weekdays_series, 'weekdays_keys.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.986522Z",
     "end_time": "2023-04-28T02:08:51.013433Z"
    }
   },
   "outputs": [],
   "source": [
    "def include_weekday(data):\n",
    "    # print('Including weekdays')\n",
    "    def get_weekday(datetime):\n",
    "        return datetime.weekday()\n",
    "\n",
    "    weekdays = data['datetime'].apply(get_weekday)\n",
    "    data['weekday'] = weekdays\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:50.992933Z",
     "end_time": "2023-04-28T02:08:51.109690Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = include_weekday(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка всего набора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс для ключей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:51.036135Z",
     "end_time": "2023-04-28T02:08:51.109851Z"
    }
   },
   "outputs": [],
   "source": [
    "class KeyDict:\n",
    "    def __init__(self):\n",
    "        self.max = 0\n",
    "        self.dict = {}\n",
    "\n",
    "    def push(self, obj):\n",
    "        if obj in self.dict:\n",
    "            # print(self.dict[obj])\n",
    "            return self.dict[obj]\n",
    "        self.dict[obj] = self.max\n",
    "        self.max += 1\n",
    "        # print(self.max - 1)\n",
    "        return self.max - 1\n",
    "\n",
    "    def get(self, obj):\n",
    "        if obj not in self.dict:\n",
    "            return None\n",
    "        return self.dict[obj]\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, 'w') as file:\n",
    "            file.write(json.dumps(self.dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:51.036250Z",
     "end_time": "2023-04-28T02:08:51.109918Z"
    }
   },
   "outputs": [],
   "source": [
    "NROWS = 200_000\n",
    "BATCH_SIZE = 200_000\n",
    "PATH = 'datasets/receipts_new.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:51.036351Z",
     "end_time": "2023-04-28T02:08:51.109971Z"
    }
   },
   "outputs": [],
   "source": [
    "# def process_batch(batch, trans_id, gid_id, types_id):\n",
    "#     time_batch_0 = time.perf_counter()\n",
    "#     batch = collapse(batch)\n",
    "#     batch, trans_id = transform_transaction_key(batch, trans_id)\n",
    "#     batch, gid_id = transform_gid(batch, gid_id)\n",
    "#     batch, types_id = transform_line_type(batch, types_id)\n",
    "#     batch = include_categories(batch, 'datasets/ProductsModels.csv')\n",
    "#     batch = include_weekday(batch)\n",
    "#\n",
    "#     time_batch_delta = time.perf_counter() - time_batch_0\n",
    "#     print('Batch time - {}'.format(time_batch_delta))\n",
    "#     return batch, trans_id, gid_id, types_id\n",
    "#\n",
    "#\n",
    "# def process_data(batch_size=BATCH_SIZE, path=PATH):\n",
    "#     batches = pd.DataFrame()\n",
    "#     trans_id = KeyDict()\n",
    "#     gid_id = KeyDict()\n",
    "#     types_id = KeyDict()\n",
    "#\n",
    "#     def concat_batches(batch, tr_id, gi_id, ty_id):\n",
    "#         nonlocal batches, trans_id, gid_id, types_id\n",
    "#\n",
    "#         print('\\n\\n' + batch.info() + '\\n\\n')\n",
    "#         trans_id, gid_id, types_id = tr_id, gi_id, ty_id\n",
    "#         batches = pd.concat([batches, batch], ignore_index=True, sort=False)\n",
    "#\n",
    "#     data = pd.read_csv(path, nrows=NROWS)\n",
    "#     data = data.rename(columns={'line_item_id': 'product_id'}).drop(columns=['opened_date', 'line_margin'])\n",
    "#\n",
    "#\n",
    "#     weekdays_id = KeyDict()\n",
    "#     weekdays_id.push('Воскресенье')\n",
    "#     weekdays_id.push('Понедельник')\n",
    "#     weekdays_id.push('Втроник')\n",
    "#     weekdays_id.push('Среда')\n",
    "#     weekdays_id.push('Четверг')\n",
    "#     weekdays_id.push('Пятница')\n",
    "#     weekdays_id.push('Суббота')\n",
    "#\n",
    "#\n",
    "#     pool = multiprocessing.Pool(processes=multiprocessing.cpu_count() - 1)\n",
    "#\n",
    "#     for batch in np.array_split(data, data.shape[0] / batch_size):\n",
    "#         pool.apply_async(\n",
    "#             process_batch,\n",
    "#             args=(batch, trans_id, gid_id, types_id),\n",
    "#             callback=concat_batches\n",
    "#         )\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "#\n",
    "#     batches = collapse(batches)\n",
    "#     batches, trans_id = transform_transaction_key(batches, trans_id)\n",
    "#     batches, gid_id = transform_gid(batches, gid_id)\n",
    "#     batches, types_id = transform_line_type(batches, types_id)\n",
    "#\n",
    "#     return batches, trans_id, gid_id, types_id, weekdays_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    global trans_id, gid_id, types_id\n",
    "    time_batch_0 = time.perf_counter()\n",
    "    # batch = batch.rename(columns = {'line_item_id': 'product_id'})\n",
    "    batch = collapse(batch)\n",
    "    batch, trans_id = transform_transaction_key(batch, trans_id)\n",
    "    batch, gid_id = transform_gid(batch, gid_id)\n",
    "    batch, types_id = transform_line_type(batch, types_id)\n",
    "    batch = include_categories(batch, 'datasets/ProductsModels.csv')\n",
    "    batch = include_weekday(batch)\n",
    "\n",
    "    time_batch_delta = time.perf_counter() - time_batch_0\n",
    "    print('Batch time - {}'.format(time_batch_delta))\n",
    "    return batch\n",
    "\n",
    "\n",
    "def process_data(batch_size=BATCH_SIZE, path=PATH):\n",
    "    global trans_id, gid_id, types_id\n",
    "    batches = pd.DataFrame()\n",
    "\n",
    "    def concat_batches(batch):\n",
    "        nonlocal batches\n",
    "        print(trans_id.max)\n",
    "        batches = pd.concat([batches, batch], ignore_index=True, sort=False)\n",
    "\n",
    "    data = pd.read_csv(path, nrows=NROWS)\n",
    "    data = data.rename(columns={'line_item_id': 'product_id'}).drop(columns=['opened_date', 'line_margin'])\n",
    "\n",
    "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
    "\n",
    "    for batch in np.array_split(data, data.shape[0] / batch_size):\n",
    "        pool.apply_async(\n",
    "            process_batch,\n",
    "            args=(batch,),\n",
    "            callback=concat_batches\n",
    "        )\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    batches = batches.rename(columns = {'key_id': 'transaction_key'})\n",
    "    batches = collapse(batches)\n",
    "\n",
    "    return batches"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T02:08:51.036535Z",
     "end_time": "2023-04-28T02:08:51.110020Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-27T17:20:29.368339Z",
     "end_time": "2023-04-27T17:24:56.492892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch time - 10.59712091900019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7 (_handle_results):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 595, in _handle_results\n",
      "    cache[job]._set(i, obj)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 779, in _set\n",
      "    self._callback(self._value)\n",
      "TypeError: process_data.<locals>.concat_batches() missing 3 required positional arguments: 'tr_id', 'gi_id', and 'ty_id'\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-10:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m time_0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[0;32m----> 2\u001B[0m data_processed, trans_id, gid_id, types_id, weekdays_id \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m time_delta \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m time_0\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOverall time - \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(time_delta))\n",
      "Cell \u001B[0;32mIn[22], line 51\u001B[0m, in \u001B[0;36mprocess_data\u001B[0;34m(batch_size, path)\u001B[0m\n\u001B[1;32m     45\u001B[0m     pool\u001B[38;5;241m.\u001B[39mapply_async(\n\u001B[1;32m     46\u001B[0m         process_batch,\n\u001B[1;32m     47\u001B[0m         args\u001B[38;5;241m=\u001B[39m(batch, trans_id, gid_id, types_id),\n\u001B[1;32m     48\u001B[0m         callback\u001B[38;5;241m=\u001B[39mconcat_batches\n\u001B[1;32m     49\u001B[0m     )\n\u001B[1;32m     50\u001B[0m pool\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m---> 51\u001B[0m \u001B[43mpool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m batches \u001B[38;5;241m=\u001B[39m collapse(batches)\n\u001B[1;32m     54\u001B[0m batches, trans_id \u001B[38;5;241m=\u001B[39m transform_transaction_key(batches, trans_id)\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:665\u001B[0m, in \u001B[0;36mPool.join\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    663\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (CLOSE, TERMINATE):\n\u001B[1;32m    664\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn unknown state\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 665\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_worker_handler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_handler\u001B[38;5;241m.\u001B[39mjoin()\n\u001B[1;32m    667\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result_handler\u001B[38;5;241m.\u001B[39mjoin()\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:1096\u001B[0m, in \u001B[0;36mThread.join\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1093\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot join current thread\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1095\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1096\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait_for_tstate_lock\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1098\u001B[0m     \u001B[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m     \u001B[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait_for_tstate_lock(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmax\u001B[39m(timeout, \u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:1116\u001B[0m, in \u001B[0;36mThread._wait_for_tstate_lock\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m   1113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   1115\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1116\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mlock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1117\u001B[0m         lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m   1118\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "time_0 = time.perf_counter()\n",
    "trans_id, gid_id, types_id, weekdays_id = KeyDict(), KeyDict(), KeyDict(), KeyDict()\n",
    "weekdays_id.push('Воскресенье')\n",
    "weekdays_id.push('Понедельник')\n",
    "weekdays_id.push('Вторник')\n",
    "weekdays_id.push('Среда')\n",
    "weekdays_id.push('Четверг')\n",
    "weekdays_id.push('Пятница')\n",
    "weekdays_id.push('Суббота')\n",
    "\n",
    "data_processed = process_data()\n",
    "time_delta = time.perf_counter() - time_0\n",
    "print('Overall time - {}'.format(time_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(multiprocessing.cpu_count())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T01:49:15.948742Z",
     "end_time": "2023-04-28T01:49:15.954350Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_processed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T17:24:56.504446Z",
     "end_time": "2023-04-27T17:24:56.529665Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-27T17:24:56.515766Z",
     "end_time": "2023-04-27T17:24:56.530050Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_id.save(DICTS_PATH + 'transaction_keys.json')\n",
    "gid_id.save(DICTS_PATH + 'gids.json')\n",
    "types_id.save(DICTS_PATH + 'type_lines.json')\n",
    "weekdays_id.save(DICTS_PATH + 'weekdays.json')\n",
    "save(data_processed, 'data_processed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
